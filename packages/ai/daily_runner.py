"""
æ¯æ—¥/æ¯å‘¨å®šæ—¶ä»»åŠ¡ç¼–æŽ’ - æ™ºèƒ½è°ƒåº¦ + ç²¾è¯»é™é¢
@author Bamzc
@author Color2333
"""

from __future__ import annotations

import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional
from uuid import UUID

from packages.ai.brief_service import DailyBriefService
from packages.ai.graph_service import GraphService
from packages.ai.pipelines import PaperPipelines
from packages.ai.rate_limiter import acquire_api, get_rate_limiter
from packages.config import get_settings
from packages.domain.enums import ActionType, ReadStatus
from packages.storage.db import session_scope
from packages.storage.models import TopicSubscription
from packages.storage.repositories import (
    PaperRepository,
    TopicRepository,
)

logger = logging.getLogger(__name__)


PAPER_CONCURRENCY = 3


def _process_paper(
    paper_id, force_deep: bool = False, deep_read_quota: Optional[int] = None
) -> dict:
    """
    å•ç¯‡è®ºæ–‡ï¼šembed âˆ¥ skim å¹¶è¡Œï¼Œæ™ºèƒ½ç²¾è¯»

    Args:
        paper_id: è®ºæ–‡ ID
        force_deep: æ˜¯å¦å¼ºåˆ¶ç²¾è¯»ï¼ˆå¿½ç•¥é…é¢ï¼‰
        deep_read_quota: å‰©ä½™ç²¾è¯»é…é¢ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶ï¼‰

    Returns:
        dict: å¤„ç†ç»“æžœ {skim_score, deep_read, success}
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    settings = get_settings()
    pipelines = PaperPipelines()
    result = {
        "paper_id": str(paper_id)[:8],
        "skim_score": None,
        "deep_read": False,
        "success": False,
        "error": None,
    }

    skim_result = None
    with ThreadPoolExecutor(max_workers=2) as inner:
        fe = inner.submit(pipelines.embed_paper, paper_id)
        fs = inner.submit(pipelines.skim, paper_id)
        for fut in as_completed([fe, fs]):
            try:
                r = fut.result()
                if fut is fs:
                    skim_result = r
            except Exception as exc:
                label = "embed" if fut is fe else "skim"
                logger.warning(
                    "%s %s failed: %s",
                    label,
                    str(paper_id)[:8],
                    exc,
                )
                result["error"] = f"{label}: {exc}"

    # æ£€æŸ¥ç²—è¯»ç»“æžœ
    if skim_result and skim_result.relevance_score is not None:
        result["skim_score"] = skim_result.relevance_score
        result["success"] = True

    # åˆ¤æ–­æ˜¯å¦ç²¾è¯»
    should_deep = False
    deep_reason = ""

    if force_deep:
        should_deep = True
        deep_reason = "å¼ºåˆ¶ç²¾è¯»"
    elif skim_result and skim_result.relevance_score >= settings.skim_score_threshold:
        # æ£€æŸ¥ç²¾è¯»é…é¢
        if deep_read_quota is None or deep_read_quota > 0:
            should_deep = True
            deep_reason = f"é«˜åˆ†è®ºæ–‡ (åˆ†æ•°={skim_result.relevance_score:.2f})"
        else:
            deep_reason = "ç²¾è¯»é…é¢å·²ç”¨å°½"

    # æ‰§è¡Œç²¾è¯»
    if should_deep:
        try:
            # èŽ·å– API è®¸å¯
            if acquire_api("llm", timeout=30.0):
                pipelines.deep_dive(UUID(paper_id))
                result["deep_read"] = True
                logger.info("ðŸŽ¯ %s ç²¾è¯»å®Œæˆ - %s", str(paper_id)[:8], deep_reason)
            else:
                logger.warning("âš ï¸  %s ç­‰å¾… API è®¸å¯è¶…æ—¶ï¼Œè·³è¿‡ç²¾è¯»", str(paper_id)[:8])
        except Exception as exc:
            logger.warning(
                "deep_dive %s failed: %s",
                str(paper_id)[:8],
                exc,
            )
            result["error"] = f"deep: {exc}"

    return result


def run_topic_ingest(topic_id: str) -> dict:
    """
    å•ç‹¬å¤„ç†ä¸€ä¸ªä¸»é¢˜çš„æŠ“å– + å¤„ç† - æ™ºèƒ½ç²¾è¯»é™é¢

    Args:
        topic_id: ä¸»é¢˜ ID

    Returns:
        dict: å¤„ç†ç»“æžœç»Ÿè®¡
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    pipelines = PaperPipelines()
    with session_scope() as session:
        topic = session.get(TopicSubscription, topic_id)
        if not topic:
            return {"topic_id": topic_id, "status": "not_found"}
        topic_name = topic.name

        # èŽ·å–ç²¾è¯»é…é¢é…ç½®
        max_deep_reads = getattr(topic, "max_deep_reads_per_run", 2)

        last_error: str | None = None
        ids: list[str] = []
        attempts = 0
        for _attempt in range(topic.retry_limit + 1):
            attempts += 1
            try:
                ids = pipelines.ingest_arxiv_with_ids(
                    query=topic.query,
                    max_results=topic.max_results_per_run,
                    topic_id=topic.id,
                    action_type=ActionType.auto_collect,
                )
                last_error = None
                break
            except Exception as exc:
                last_error = str(exc)

        if last_error is not None:
            return {
                "topic_id": topic_id,
                "topic_name": topic_name,
                "status": "failed",
                "attempts": attempts,
                "error": last_error,
                "inserted": 0,
            }

        repo = PaperRepository(session)
        # åªå¤„ç†è¿™æ¬¡æ–°å…¥åº“çš„è®ºæ–‡
        unique = repo.list_by_ids(ids) if ids else []
        # åœ¨ Session å…³é—­å‰æå–æ‰€æœ‰éœ€è¦çš„æ•°æ®ï¼Œé¿å… DetachedInstanceError
        papers_data = [(str(p.id), p.title) for p in unique]

    logger.info(
        "ðŸ“ ä¸»é¢˜ [%s] æ–°æŠ“å– %d ç¯‡è®ºæ–‡ï¼Œç²¾è¯»é…é¢ï¼š%d ç¯‡", topic_name, len(unique), max_deep_reads
    )

    # ç¬¬ä¸€æ­¥ï¼šå…¨éƒ¨è®ºæ–‡å¹¶è¡Œç²—è¯» + åµŒå…¥ï¼ˆä¸ç²¾è¯»ï¼‰
    logger.info("ç¬¬ä¸€æ­¥ï¼šå¹¶è¡Œç²—è¯» + åµŒå…¥...")
    skim_results = []

    with ThreadPoolExecutor(max_workers=PAPER_CONCURRENCY) as pool:
        futs = {
            pool.submit(_process_paper, paper_id, force_deep=False, deep_read_quota=0): paper_id
            for paper_id, _ in papers_data
        }
        for fut in as_completed(futs):
            try:
                result = fut.result()
                skim_results.append(result)
            except Exception as exc:
                paper_id = futs[fut]
                logger.warning(
                    "skim %s failed: %s",
                    str(paper_id)[:8],
                    exc,
                )

    # ç¬¬äºŒæ­¥ï¼šæŒ‰ç²—è¯»åˆ†æ•°æŽ’åºï¼Œé€‰å‰ N ç¯‡ç²¾è¯»
    logger.info("ç¬¬äºŒæ­¥ï¼šé€‰æ‹©é«˜åˆ†è®ºæ–‡è¿›è¡Œç²¾è¯»...")
    # åªç”¨ ID å’Œåˆ†æ•°æŽ’åºï¼Œä¸å†å¼•ç”¨ ORM å¯¹è±¡
    scored_papers = [
        (r, paper_id)
        for r, (paper_id, _) in zip(skim_results, papers_data)
        if r["success"] and r["skim_score"] is not None
    ]
    scored_papers.sort(key=lambda x: x[0]["skim_score"], reverse=True)

    # ç²¾è¯»å‰ N ç¯‡
    deep_read_count = 0
    for i, (result, paper_id) in enumerate(scored_papers):
        if deep_read_count >= max_deep_reads:
            logger.info(
                "âš ï¸  ç²¾è¯»é…é¢å·²ç”¨å°½ (%d/%d)ï¼Œå‰©ä½™ %d ç¯‡è·³è¿‡ç²¾è¯»",
                deep_read_count,
                max_deep_reads,
                len(scored_papers) - i,
            )
            break

        # åªç²¾è¯»åˆ†æ•° >= é˜ˆå€¼çš„
        if result["skim_score"] < get_settings().skim_score_threshold:
            logger.info("âš ï¸  %s åˆ†æ•°è¿‡ä½Ž (%.2f)ï¼Œè·³è¿‡ç²¾è¯»", str(paper_id)[:8], result["skim_score"])
            continue

        logger.info(
            "ðŸŽ¯ å¼€å§‹ç²¾è¯»ç¬¬ %d ç¯‡ï¼š%s (åˆ†æ•°=%.2f)",
            deep_read_count + 1,
            str(paper_id)[:50],
            result["skim_score"],
        )

        try:
            # èŽ·å– API è®¸å¯
            if acquire_api("llm", timeout=60.0):
                pipelines.deep_dive(UUID(paper_id))  # type: ignore[arg-type]
                deep_read_count += 1
                logger.info("âœ… ç²¾è¯»å®Œæˆ (%d/%d)", deep_read_count, max_deep_reads)
            else:
                logger.warning("ç­‰å¾… API è®¸å¯è¶…æ—¶ï¼Œè·³è¿‡ç²¾è¯»")
        except Exception as exc:
            logger.warning(
                "deep_dive %s failed: %s",
                str(paper_id)[:8],
                exc,
            )

    return {
        "topic_id": topic_id,
        "topic_name": topic_name,
        "status": "ok",
        "attempts": attempts,
        "inserted": len(ids),
        "skimmed": len(skim_results),
        "deep_read": deep_read_count,
        "max_deep_reads": max_deep_reads,
    }


def run_daily_ingest() -> dict:
    """å…¼å®¹æ—§è°ƒç”¨ï¼šéåŽ†æ‰€æœ‰ enabled ä¸»é¢˜æ‰§è¡ŒæŠ“å–"""
    with session_scope() as session:
        topic_repo = TopicRepository(session)
        topics = topic_repo.list_topics(enabled_only=True)
        if not topics:
            topics = [
                topic_repo.upsert_topic(
                    name="default-ml",
                    query="cat:cs.LG OR cat:cs.CL",
                    enabled=True,
                    max_results_per_run=20,
                    retry_limit=2,
                )
            ]
        topic_ids = [t.id for t in topics]

    results = []
    for tid in topic_ids:
        results.append(run_topic_ingest(tid))

    total_inserted = sum(r.get("inserted", 0) for r in results)
    total_processed = sum(r.get("processed", 0) for r in results)
    return {
        "newly_inserted": total_inserted,
        "processed": total_processed,
        "topics": results,
    }


def run_daily_brief() -> dict:
    settings = get_settings()
    return DailyBriefService().publish(recipient=settings.notify_default_to)


def run_weekly_graph_maintenance() -> dict:
    with session_scope() as session:
        topics = TopicRepository(session).list_topics(enabled_only=True)
    graph = GraphService()
    topic_results = []
    for t in topics:
        try:
            topic_results.append(
                graph.sync_citations_for_topic(
                    topic_id=t.id,
                    paper_limit=20,
                    edge_limit_per_paper=6,
                )
            )
        except Exception:
            logger.exception(
                "Failed to sync citations for topic %s",
                t.id,
            )
            continue
    incremental = graph.sync_incremental(paper_limit=50, edge_limit_per_paper=6)
    return {
        "topic_sync": topic_results,
        "incremental": incremental,
    }
