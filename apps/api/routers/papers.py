"""è®ºæ–‡ç®¡ç†è·¯ç”±
@author Color2333
"""

from pathlib import Path
from uuid import UUID

from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import FileResponse

from apps.api.deps import cache, paper_list_response, rag_service
from packages.domain.schemas import AIExplainReq
from packages.domain.task_tracker import global_tracker
from packages.storage.db import session_scope
from packages.storage.repositories import PaperRepository

router = APIRouter()


@router.get("/papers/folder-stats")
def paper_folder_stats() -> dict:
    """è®ºæ–‡æ–‡ä»¶å¤¹ç»Ÿè®¡ï¼ˆ30s ç¼“å­˜ï¼‰"""
    cached = cache.get("folder_stats")
    if cached is not None:
        return cached
    with session_scope() as session:
        repo = PaperRepository(session)
        result = repo.folder_stats()
    cache.set("folder_stats", result, ttl=30)
    return result


@router.get("/papers/latest")
def latest(
    limit: int = Query(default=50, ge=1, le=500),
    page: int = Query(default=1, ge=1),
    page_size: int = Query(default=20, ge=1, le=100),
    status: str | None = Query(default=None),
    topic_id: str | None = Query(default=None),
    folder: str | None = Query(default=None),
    date: str | None = Query(default=None),
    search: str | None = Query(default=None),
) -> dict:
    with session_scope() as session:
        repo = PaperRepository(session)
        papers, total = repo.list_paginated(
            page=page,
            page_size=page_size,
            folder=folder,
            topic_id=topic_id,
            status=status,
            date_str=date,
            search=search.strip() if search else None,
        )
        resp = paper_list_response(papers, repo)
        resp["total"] = total
        resp["page"] = page
        resp["page_size"] = page_size
        resp["total_pages"] = max(1, (total + page_size - 1) // page_size)
        return resp


@router.get("/papers/recommended")
def recommended_papers(top_k: int = Query(default=10, ge=1, le=50)) -> dict:
    from packages.ai.recommendation_service import RecommendationService

    return {"items": RecommendationService().recommend(top_k=top_k)}


@router.get("/papers/proxy-arxiv-pdf/{arxiv_id:path}")
async def proxy_arxiv_pdf(arxiv_id: str):
    """ä»£ç†è®¿é—® arXiv PDFï¼ˆè§£å†³ CORS é—®é¢˜ï¼‰"""
    import httpx

    # æ¸…ç† arxiv_idï¼ˆç§»é™¤ç‰ˆæœ¬å·ï¼‰
    clean_id = arxiv_id.split("v")[0]
    arxiv_url = f"https://arxiv.org/pdf/{clean_id}.pdf"

    try:
        # ä½¿ç”¨åç«¯æœåŠ¡å™¨è®¿é—® arXivï¼ˆç»•è¿‡ CORSï¼‰
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.get(arxiv_url, follow_redirects=True)

            if response.status_code == 404:
                raise HTTPException(status_code=404, detail=f"arXiv è®ºæ–‡ä¸å­˜åœ¨ï¼š{clean_id}")

            if response.status_code != 200:
                raise HTTPException(
                    status_code=500, detail=f"arXiv è®¿é—®å¤±è´¥ï¼š{response.status_code}"
                )

            # è¿”å› PDF å†…å®¹
            from fastapi.responses import Response

            return Response(
                content=response.content,
                media_type="application/pdf",
                headers={
                    "Access-Control-Allow-Origin": "*",
                    "Content-Disposition": f'inline; filename="{clean_id}.pdf"',
                    "Cache-Control": "public, max-age=3600",
                },
            )
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="arXiv è¯·æ±‚è¶…æ—¶")
    except httpx.RequestError as exc:
        raise HTTPException(status_code=500, detail=f"arXiv è®¿é—®å¤±è´¥ï¼š{str(exc)}")


@router.get("/papers/{paper_id}")
def paper_detail(paper_id: UUID) -> dict:
    with session_scope() as session:
        repo = PaperRepository(session)
        try:
            p = repo.get_by_id(paper_id)
        except ValueError as exc:
            raise HTTPException(status_code=404, detail=str(exc)) from exc
        topic_map = repo.get_topic_names_for_papers([str(p.id)])
        # æŸ¥è¯¢å·²æœ‰åˆ†ææŠ¥å‘Š
        from packages.storage.models import AnalysisReport as AR
        from sqlalchemy import select as _sel

        ar = session.execute(_sel(AR).where(AR.paper_id == str(p.id))).scalar_one_or_none()
        skim_data = None
        deep_data = None
        if ar:
            if ar.summary_md:
                skim_data = {
                    "summary_md": ar.summary_md,
                    "skim_score": ar.skim_score,
                    "key_insights": ar.key_insights or {},
                }
            if ar.deep_dive_md:
                deep_data = {
                    "deep_dive_md": ar.deep_dive_md,
                    "key_insights": ar.key_insights or {},
                }
        return {
            "id": str(p.id),
            "title": p.title,
            "arxiv_id": p.arxiv_id,
            "abstract": p.abstract,
            "publication_date": str(p.publication_date) if p.publication_date else None,
            "read_status": p.read_status.value,
            "pdf_path": p.pdf_path,
            "favorited": getattr(p, "favorited", False),
            "categories": (p.metadata_json or {}).get("categories", []),
            "authors": (p.metadata_json or {}).get("authors", []),
            "keywords": (p.metadata_json or {}).get("keywords", []),
            "title_zh": (p.metadata_json or {}).get("title_zh", ""),
            "abstract_zh": (p.metadata_json or {}).get("abstract_zh", ""),
            "topics": topic_map.get(str(p.id), []),
            "metadata": p.metadata_json,
            "has_embedding": p.embedding is not None,
            "skim_report": skim_data,
            "deep_report": deep_data,
        }


@router.patch("/papers/{paper_id}/favorite")
def toggle_favorite(paper_id: UUID) -> dict:
    """åˆ‡æ¢è®ºæ–‡æ”¶è—çŠ¶æ€"""
    with session_scope() as session:
        repo = PaperRepository(session)
        try:
            p = repo.get_by_id(paper_id)
        except ValueError as exc:
            raise HTTPException(status_code=404, detail=str(exc)) from exc
        current = getattr(p, "favorited", False)
        p.favorited = not current
        session.commit()
        cache.invalidate("folder_stats")
        return {"id": str(p.id), "favorited": p.favorited}


# ---------- PDF æœåŠ¡ ----------


@router.post("/papers/{paper_id}/download-pdf")
def download_paper_pdf(paper_id: UUID) -> dict:
    """ä» arXiv ä¸‹è½½è®ºæ–‡ PDF"""
    from packages.integrations.arxiv_client import ArxivClient

    with session_scope() as session:
        repo = PaperRepository(session)
        try:
            paper = repo.get_by_id(paper_id)
        except ValueError as exc:
            raise HTTPException(status_code=404, detail=str(exc)) from exc
        if paper.pdf_path and Path(paper.pdf_path).exists():
            return {"status": "exists", "pdf_path": paper.pdf_path}
        if not paper.arxiv_id or paper.arxiv_id.startswith("ss-"):
            raise HTTPException(status_code=400, detail="è¯¥è®ºæ–‡æ²¡æœ‰æœ‰æ•ˆçš„ arXiv IDï¼Œæ— æ³•ä¸‹è½½ PDF")
        try:
            pdf_path = ArxivClient().download_pdf(paper.arxiv_id)
            repo.set_pdf_path(paper_id, pdf_path)
            return {"status": "downloaded", "pdf_path": pdf_path}
        except Exception as exc:
            raise HTTPException(status_code=500, detail=f"PDF ä¸‹è½½å¤±è´¥: {exc}") from exc


@router.get("/papers/{paper_id}/pdf")
def serve_paper_pdf(paper_id: UUID) -> FileResponse:
    """æä¾›è®ºæ–‡ PDF æ–‡ä»¶ä¸‹è½½/é¢„è§ˆ"""
    with session_scope() as session:
        repo = PaperRepository(session)
        try:
            paper = repo.get_by_id(paper_id)
        except ValueError as exc:
            raise HTTPException(status_code=404, detail=str(exc)) from exc
        pdf_path = paper.pdf_path
    if not pdf_path:
        raise HTTPException(status_code=404, detail="è®ºæ–‡æ²¡æœ‰ PDF æ–‡ä»¶")
    full_path = Path(pdf_path)
    if not full_path.exists():
        raise HTTPException(status_code=404, detail="PDF æ–‡ä»¶ä¸å­˜åœ¨")
    return FileResponse(
        path=str(full_path),
        media_type="application/pdf",
        headers={"Access-Control-Allow-Origin": "*"},
    )


@router.post("/papers/{paper_id}/ai/explain")
def ai_explain_text(paper_id: UUID, body: AIExplainReq) -> dict:
    """AI è§£é‡Š/ç¿»è¯‘é€‰ä¸­æ–‡æœ¬"""
    text = body.text.strip()
    action = body.action
    if not text:
        raise HTTPException(status_code=400, detail="text is required")

    prompts = {
        "explain": (
            f"ä½ æ˜¯å­¦æœ¯è®ºæ–‡è§£è¯»ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡ç®€æ´è§£é‡Šä»¥ä¸‹å­¦æœ¯æ–‡æœ¬çš„å«ä¹‰ï¼Œ"
            f"åŒ…æ‹¬ä¸“ä¸šæœ¯è¯­è§£é‡Šå’Œæ ¸å¿ƒæ„æ€ã€‚å¦‚æœæ˜¯å…¬å¼ï¼Œè§£é‡Šå…¬å¼çš„å«ä¹‰å’Œå„å˜é‡ã€‚\n\n"
            f"æ–‡æœ¬ï¼š{text[:2000]}"
        ),
        "translate": (
            f"è¯·å°†ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬ç¿»è¯‘ä¸ºæµç•…çš„ä¸­æ–‡ï¼Œä¿ç•™ä¸“ä¸šæœ¯è¯­çš„è‹±æ–‡åŸæ–‡ï¼ˆæ‹¬å·æ ‡æ³¨ï¼‰ã€‚\n\n"
            f"æ–‡æœ¬ï¼š{text[:2000]}"
        ),
        "summarize": (f"è¯·ç”¨ä¸­æ–‡ç®€è¦æ€»ç»“ä»¥ä¸‹å†…å®¹çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆ3-5 å¥è¯ï¼‰ï¼š\n\n{text[:3000]}"),
    }
    prompt = prompts.get(action, prompts["explain"])

    from packages.integrations.llm_client import LLMClient

    llm = LLMClient()
    result = llm.summarize_text(prompt, stage="rag", max_tokens=1024)
    llm.trace_result(
        result, stage="pdf_reader_ai", prompt_digest=f"{action}:{text[:80]}", paper_id=str(paper_id)
    )
    return {"action": action, "result": result.content}


# ---------- å›¾è¡¨è§£è¯» ----------


@router.get("/papers/{paper_id}/figures")
def get_paper_figures(paper_id: UUID) -> dict:
    """è·å–è®ºæ–‡å·²æœ‰çš„å›¾è¡¨è§£è¯»"""
    from packages.ai.figure_service import FigureService

    items = FigureService.get_paper_analyses(paper_id)
    for item in items:
        if item.get("has_image"):
            item["image_url"] = f"/papers/{paper_id}/figures/{item['id']}/image"
        else:
            item["image_url"] = None
    return {"items": items}


@router.get("/papers/{paper_id}/figures/{figure_id}/image")
def get_figure_image(paper_id: UUID, figure_id: str):
    """è¿”å›å›¾è¡¨åŸå§‹å›¾ç‰‡æ–‡ä»¶"""
    from packages.storage.db import session_scope
    from packages.storage.models import ImageAnalysis
    from sqlalchemy import select

    with session_scope() as session:
        row = session.execute(
            select(ImageAnalysis).where(
                ImageAnalysis.id == figure_id,
                ImageAnalysis.paper_id == str(paper_id),
            )
        ).scalar_one_or_none()

        if not row or not row.image_path:
            raise HTTPException(status_code=404, detail="å›¾ç‰‡ä¸å­˜åœ¨")

        img_path = Path(row.image_path)
        if not img_path.exists():
            raise HTTPException(status_code=404, detail="å›¾ç‰‡æ–‡ä»¶ä¸¢å¤±")

        return FileResponse(img_path, media_type="image/png")


@router.post("/papers/{paper_id}/figures/analyze")
def analyze_paper_figures(
    paper_id: UUID,
    max_figures: int = Query(default=10, ge=1, le=30),
) -> dict:
    """æå–å¹¶è§£è¯»è®ºæ–‡ä¸­çš„å›¾è¡¨ï¼ˆå¼‚æ­¥ä»»åŠ¡ï¼‰"""
    from packages.domain.task_tracker import global_tracker

    # å…ˆéªŒè¯è®ºæ–‡å’Œ PDF
    with session_scope() as session:
        repo = PaperRepository(session)
        try:
            paper = repo.get_by_id(paper_id)
        except ValueError as exc:
            raise HTTPException(status_code=404, detail=str(exc)) from exc
        if not paper.pdf_path:
            raise HTTPException(status_code=400, detail="è®ºæ–‡æ²¡æœ‰ PDF æ–‡ä»¶")
        pdf_path = paper.pdf_path
        paper_title = paper.title[:50]

    # æäº¤åå°ä»»åŠ¡
    def _analyze_fn(progress_callback=None):
        from packages.ai.figure_service import FigureService

        svc = FigureService()
        results = svc.analyze_paper_figures(paper_id, pdf_path, max_figures)
        # åˆ†æå®Œæˆåï¼Œä» DB è·å–å¸¦ id çš„å®Œæ•´ç»“æœ
        from packages.ai.figure_service import FigureService as FS2

        items = FS2.get_paper_analyses(paper_id)
        for item in items:
            if item.get("has_image"):
                item["image_url"] = f"/papers/{paper_id}/figures/{item['id']}/image"
            else:
                item["image_url"] = None
        return {"paper_id": str(paper_id), "count": len(items), "items": items}

    task_id = global_tracker.submit(
        task_type="figure_analysis",
        title=f"ğŸ“Š å›¾è¡¨åˆ†æï¼š{paper_title}",
        fn=_analyze_fn,
        total=max_figures,
    )
    return {
        "task_id": task_id,
        "status": "started",
        "message": "å›¾è¡¨åˆ†æå·²å¯åŠ¨ï¼Œæ­£åœ¨å¤„ç†...",
    }


@router.get("/papers/{paper_id}/similar")
def similar(
    paper_id: UUID,
    top_k: int = Query(default=5, ge=1, le=20),
) -> dict:
    ids = rag_service.similar_papers(paper_id, top_k=top_k)
    items = []
    if ids:
        with session_scope() as session:
            repo = PaperRepository(session)
            for pid in ids:
                try:
                    p = repo.get_by_id(pid)
                    items.append(
                        {
                            "id": str(p.id),
                            "title": p.title,
                            "arxiv_id": p.arxiv_id,
                            "read_status": p.read_status.value if p.read_status else "unread",
                        }
                    )
                except Exception:
                    items.append(
                        {
                            "id": str(pid),
                            "title": str(pid),
                            "arxiv_id": None,
                            "read_status": "unread",
                        }
                    )
    return {
        "paper_id": str(paper_id),
        "similar_ids": [str(x) for x in ids],
        "items": items,
    }


@router.post("/papers/{paper_id}/reasoning")
def paper_reasoning(paper_id: UUID) -> dict:
    """æ¨ç†é“¾æ·±åº¦åˆ†æ"""
    from packages.ai.reasoning_service import ReasoningService

    with session_scope() as session:
        repo = PaperRepository(session)
        try:
            repo.get_by_id(paper_id)
        except ValueError as exc:
            raise HTTPException(status_code=404, detail=str(exc)) from exc
    return ReasoningService().analyze(paper_id)
