#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†æœªè¯»è®ºæ–‡ - ç²—è¯» + åµŒå…¥
@author Color2333
"""

from __future__ import annotations

import argparse
import logging
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

from packages.ai.pipelines import PaperPipelines
from packages.config import get_settings
from packages.storage.db import session_scope
from packages.storage.models import Paper, AnalysisReport
from packages.storage.repositories import AnalysisRepository
from sqlalchemy import select

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)


def get_unread_papers(limit: int = 50) -> list[tuple[str, str, str]]:
    """è·å–æœªè¯»ä¸”æœªå¤„ç†çš„è®ºæ–‡åˆ—è¡¨"""
    with session_scope() as session:
        # æŸ¥è¯¢æœªè¯»è®ºæ–‡ï¼Œå·¦è¿æ¥åˆ†æè¡¨ï¼Œç­›é€‰æ²¡æœ‰åˆ†æçš„
        papers = session.execute(
            select(Paper.id, Paper.title, Paper.arxiv_id)
            .where(Paper.read_status == "unread")
            .outerjoin(AnalysisReport, Paper.id == AnalysisReport.paper_id)
            .where((AnalysisReport.summary_md.is_(None)) | (AnalysisReport.id.is_(None)))
            .order_by(Paper.created_at.desc())
            .limit(limit)
        ).all()
        return [(p.id, p.title, p.arxiv_id) for p in papers]


def process_single_paper(paper_id: str, title: str) -> dict:
    """å¤„ç†å•ç¯‡è®ºæ–‡ï¼šembed + skim å¹¶è¡Œ"""
    pipelines = PaperPipelines()
    result = {
        "paper_id": paper_id,
        "title": title[:50],
        "skim_success": False,
        "embed_success": False,
        "error": None,
    }

    try:
        # embed å’Œ skim å¹¶è¡Œæ‰§è¡Œ
        from concurrent.futures import ThreadPoolExecutor

        with ThreadPoolExecutor(max_workers=2) as executor:
            embed_future = executor.submit(pipelines.embed_paper, paper_id)
            skim_future = executor.submit(pipelines.skim, paper_id)

            # ç­‰å¾… embed å®Œæˆ
            try:
                embed_future.result()
                result["embed_success"] = True
                logger.info(f"âœ… {title[:40]}... åµŒå…¥å®Œæˆ")
            except Exception as e:
                result["error"] = f"embed: {e}"
                logger.warning(f"âŒ {title[:40]}... åµŒå…¥å¤±è´¥ï¼š{e}")

            # ç­‰å¾… skim å®Œæˆ
            try:
                skim_result = skim_future.result()
                result["skim_success"] = True
                if skim_result and skim_result.relevance_score:
                    result["relevance_score"] = skim_result.relevance_score
                logger.info(
                    f"âœ… {title[:40]}... ç²—è¯»å®Œæˆ (åˆ†æ•°ï¼š{skim_result.relevance_score if skim_result else 'N/A'})"
                )
            except Exception as e:
                result["error"] = f"skim: {e}"
                logger.warning(f"âŒ {title[:40]}... ç²—è¯»å¤±è´¥ï¼š{e}")

        # å¦‚æœç²—è¯»åˆ†æ•°é«˜ï¼Œè‡ªåŠ¨ç²¾è¯»
        if result["skim_success"] and result.get("relevance_score", 0) >= 0.65:
            try:
                pipelines.deep_dive(paper_id)
                result["deep_success"] = True
                logger.info(f"ğŸ¯ {title[:40]}... è‡ªåŠ¨ç²¾è¯»å®Œæˆ (é«˜åˆ†è®ºæ–‡)")
            except Exception as e:
                logger.warning(f"âš ï¸  {title[:40]}... ç²¾è¯»å¤±è´¥ï¼š{e}")
                result["deep_success"] = False

    except Exception as e:
        result["error"] = str(e)
        logger.exception(f"âŒ {title[:40]}... å¤„ç†å¼‚å¸¸ï¼š{e}")

    return result


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡å¤„ç†æœªè¯»è®ºæ–‡")
    parser.add_argument("--limit", type=int, default=20, help="æ¯æ¬¡å¤„ç†çš„è®ºæ–‡æ•°é‡ (é»˜è®¤ 20)")
    parser.add_argument(
        "--concurrency", type=int, default=3, help="å¹¶å‘å¤„ç†æ•°é‡ (é»˜è®¤ 3ï¼Œé¿å… LLM é™æµ)"
    )
    parser.add_argument("--auto-deep", action="store_true", help="ç²—è¯»åˆ†æ•°>=0.65 æ—¶è‡ªåŠ¨ç²¾è¯»")
    args = parser.parse_args()

    print("=" * 60)
    print("ğŸš€ PaperMind æ‰¹é‡è®ºæ–‡å¤„ç†å™¨")
    print("=" * 60)
    print()

    # è·å–å¾…å¤„ç†è®ºæ–‡
    papers = get_unread_papers(limit=args.limit)

    if not papers:
        print("âœ… æ²¡æœ‰éœ€è¦å¤„ç†çš„æœªè¯»è®ºæ–‡ï¼")
        return

    print(f"ğŸ“Š æ‰¾åˆ° {len(papers)} ç¯‡å¾…å¤„ç†è®ºæ–‡")
    print(f"âš¡ å¹¶å‘æ•°ï¼š{args.concurrency}")
    print(f"ğŸ¯ è‡ªåŠ¨ç²¾è¯»ï¼š{'å¼€å¯' if args.auto_deep else 'å…³é—­'}")
    print()

    # æ‰¹é‡å¤„ç†
    results = []
    with ThreadPoolExecutor(max_workers=args.concurrency) as executor:
        futures = {
            executor.submit(process_single_paper, pid, title): pid
            for pid, title, arxiv_id in papers
        }

        for i, future in enumerate(as_completed(futures), 1):
            result = future.result()
            results.append(result)

            # è¿›åº¦æ±‡æŠ¥
            if i % 5 == 0 or i == len(papers):
                success = sum(1 for r in results if r["skim_success"] and r["embed_success"])
                logger.info(f"è¿›åº¦ï¼š{i}/{len(papers)} | æˆåŠŸï¼š{success} | å¤±è´¥ï¼š{i - success}")

    # ç»Ÿè®¡ç»“æœ
    print()
    print("=" * 60)
    print("ğŸ“‹ å¤„ç†ç»“æœç»Ÿè®¡")
    print("=" * 60)

    total = len(results)
    skim_ok = sum(1 for r in results if r["skim_success"])
    embed_ok = sum(1 for r in results if r["embed_success"])
    deep_ok = sum(1 for r in results if r.get("deep_success", False))
    errors = sum(1 for r in results if r["error"])

    print(f"æ€»å¤„ç†ï¼š{total} ç¯‡")
    print(f"âœ… ç²—è¯»æˆåŠŸï¼š{skim_ok} ({skim_ok / total * 100:.1f}%)")
    print(f"âœ… åµŒå…¥æˆåŠŸï¼š{embed_ok} ({embed_ok / total * 100:.1f}%)")
    print(f"ğŸ¯ è‡ªåŠ¨ç²¾è¯»ï¼š{deep_ok}")
    print(f"âŒ å‡ºç°é”™è¯¯ï¼š{errors} ({errors / total * 100:.1f}%)")
    print()

    # æ˜¾ç¤ºå¤±è´¥çš„è®ºæ–‡
    if errors > 0:
        print("âš ï¸  ä»¥ä¸‹è®ºæ–‡å¤„ç†å¤±è´¥:")
        for r in results:
            if r["error"]:
                print(f"  - {r['title'][:40]}... : {r['error']}")
        print()

    print("âœ¨ æ‰¹é‡å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
